{
    "education": "Education Details \r\nJanuary 2016 Bachelors of Engineering Engineering  Gujarat Technological University\r\nSystems Engineer/Hadoop Developer \r\n\r\nSystems Engineer/Hadoop Developer - Tata Consultancy Services\r\nSkill Details \r\nHadoop,Spark,Sqoop,Hive,Flume,Pig- Exprience - 24 monthsCompany Details \r\ncompany - Tata Consultancy Services\r\ndescription - Roles and responsibility:\r\n\r\nWorking for a American pharmaceutical company (one of the world's premier\r\nbiopharmaceutical) who develops and produces medicines and vaccines for a wide range of medical\r\ndisciplines, including immunology, oncology, cardiology, endocrinology, and neurology. To handle large\r\namount of United Healthcare data big data analytics is used. Data from all possible data sources like records of all Patients(Old and New), records of medicines, Treatment Pathways & Patient Journey for\r\nHealth Outcomes, Patient Finder (or Rare Disease Patient Finder), etc being gathered, stored and processed at one place.\r\n\r\n\u00e2\u0080\u00a2     Worked on cluster with specs as:\r\no    Cluster Architecture: Fully\r\nDistributed Package Used:\r\nCDH3\r\no    Cluster Capacity: 20 TB\r\no    No. of Nodes: 10 Data Nodes + 3 Masters + NFS Backup For NN\r\n\r\n\u00e2\u0080\u00a2     Developed proof of concepts for enterprise adoption of Hadoop.\r\n\u00e2\u0080\u00a2   Used SparkAPI over Cloudera Hadoop YARN to perform analytics on the Healthcare data in Cloudera\r\ndistribution.\r\n\u00e2\u0080\u00a2   Responsible for cluster maintenance, adding and removing cluster nodes, cluster monitoring and trouble-shooting, manage and review data backups, and reviewing Hadoop log files.\r\n\u00e2\u0080\u00a2   Imported & exported large data sets of data into HDFS and vice-versa using sqoop.\r\n\u00e2\u0080\u00a2   Involved developing the Pig scripts and Hive Reports\r\n\u00e2\u0080\u00a2   Worked on Hive partition and bucketing concepts and created hive external and Internal tables with Hive\r\npartition.Monitoring Hadoop scripts which take the input from HDFS and load the data into Hive.\r\n\u00e2\u0080\u00a2   Developed Spark scripts by using Scala shell commands as per the requirement and worked with both\r\nData frames/SQL/Data sets and RDD/MapReduce in Spark. Optimizing of existing algorithms in Hadoop\r\nusing SparkContext, Spark-SQL, Data Frames and RDD's.\r\n\u00e2\u0080\u00a2   Collaborated with infrastructure, network, database, application and BI to ensure data, quality and availability.\r\n\u00e2\u0080\u00a2   Developed reports using TABLEAU and exported data to HDFS and hive using Sqoop.\r\n\u00e2\u0080\u00a2   Used ORC & Parquet file formats for serialization of data, and Snappy for the compression of the data.\r\n\r\nAchievements\r\n\r\n\u00e2\u0080\u00a2   Appreciation for showing articulate leadership qualities in doing work with the team.\r\n\u00e2\u0080\u00a2   Completed the internal certification of TCS Certified Hadoop Developer.\r\n\r\nOngoing Learning\r\n\u00e2\u0080\u00a2   Preparing and scheduled the Cloudera Certified Spark Developer CCA 175.",
    "skills": null,
    "experience": null,
    "summary": "\u00e2\u0080\u00a2 Operating systems:-Linux- Ubuntu, Windows 2007/08 \u00e2\u0080\u00a2 Other tools:- Tableau, SVN, Beyond Compare.Education Details January 2016 Bachelors of Engineering Engineering  Gujarat Technological University Systems Engineer/Hadoop Developer",
    "category": "Hadoop"
}