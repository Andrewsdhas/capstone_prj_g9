{
    "education": "Education Details \r\n\r\nHadoop Developer \r\n\r\nHadoop Developer - INFOSYS\r\nSkill Details \r\nCompany Details \r\ncompany - INFOSYS\r\ndescription - Project Description: The banking information had stored the data in different data ware house systems for each department but it becomes difficult for the organization to manage the data and to perform some analytics on the past data, so it is combined them into a single global repository in Hadoop for analysis.\r\n\r\nResponsibilities:\r\n\u00e2\u0080\u00a2       Analyze the banking rates data set.\r\n\u00e2\u0080\u00a2       Create specification document.\r\n\u00e2\u0080\u00a2       Provide effort estimation.\r\n\u00e2\u0080\u00a2       Develop SPARK Scala, SPARK SQL Programs using Eclipse IDE on Windows/Linux environment.\r\n\u00e2\u0080\u00a2       Create KPI's test scenarios, test cases, test result document.\r\n\u00e2\u0080\u00a2       Test the Scala programs in Linux Spark Standalone mode.\r\n\u00e2\u0080\u00a2       setup multi cluster on AWS, deploy the Spark Scala programs\r\n\u00e2\u0080\u00a2       Provided solution using Hadoop ecosystem - HDFS, MapReduce, Pig, Hive, HBase, and Zookeeper.\r\n\u00e2\u0080\u00a2       Provided solution using large scale server-side systems with distributed processing algorithms.\r\n\u00e2\u0080\u00a2       Created reports for the BI team using Sqoop to export data into HDFS and Hive.\r\n\u00e2\u0080\u00a2       Provided solution in supporting and assisting in troubleshooting and optimization of MapReduce jobs and\r\nPig Latin scripts.\r\n\u00e2\u0080\u00a2       Deep understanding of Hadoop design principles, cluster connectivity, security and the factors that affect\r\nsystem performance.\r\n\u00e2\u0080\u00a2       Worked on Importing and exporting data from different databases like Oracle, Teradata into HDFS and Hive\r\nusing Sqoop, TPT and Connect Direct.\r\n\u00e2\u0080\u00a2       Import and export the data from RDBMS to HDFS/HBASE\r\n\u00e2\u0080\u00a2       Wrote script and placed it in client side so that the data moved to HDFS will be stored in temporary file and then it will start loading it in hive tables.\r\n\u00e2\u0080\u00a2       Developed the Sqoop scripts in order to make the interaction between Pig and MySQL Database.\r\n\u00e2\u0080\u00a2       Involved in developing the Hive Reports, Partitions of Hive tables.\r\n\u00e2\u0080\u00a2       Created and maintained technical documentation for launching HADOOP Clusters and for executing HIVE\r\nqueries and PIG scripts.\r\n\u00e2\u0080\u00a2       Involved in running Hadoop jobs for processing millions of records of text data\r\n\r\nEnvironment: Java, Hadoop, HDFS, Map-Reduce, Pig, Hive, Sqoop, Flume, Oozie, HBase, Spark, Scala,\r\nLinux, NoSQL, Storm, Tomcat, Putty, SVN, GitHub, IBM WebSphere v8.5.\r\n\r\nProject #1: TELECOMMUNICATIONS\r\nHadoop Developer\r\n\r\nDescription To identify customers who are likely to churn and 360-degree view of the customer is created from different heterogeneous data sources. The data is brought into data lake (HDFS) from different sources and analyzed using different Hadoop tools like pig and hive.\r\n\r\nResponsibilities:\r\n\u00e2\u0080\u00a2       Installed and Configured Apache Hadoop tools like Hive, Pig, HBase and Sqoop for application development and unit testing.\r\n\u00e2\u0080\u00a2       Wrote MapReduce jobs to discover trends in data usage by users.\r\n\u00e2\u0080\u00a2       Involved in database connection using SQOOP.\r\n\u00e2\u0080\u00a2       Involved in creating Hive tables, loading data and writing hive queries Using the HiveQL.\r\n\u00e2\u0080\u00a2       Involved in partitioning and joining Hive tables for Hive query optimization.\r\n\u00e2\u0080\u00a2       Experience",
    "skills": null,
    "experience": "Experienced in SQL DB Migration to HDFS.\r\n\u00e2\u0080\u00a2       Used NoSQL(HBase) for faster performance, which maintains the data in the De-Normalized way for OLTP.\r\n\u00e2\u0080\u00a2       The data is collected from distributed sources into Avro models. Applied transformations and standardizations and loaded into HBase for further data processing.\r\n\u00e2\u0080\u00a2       Experienced in defining job flows.\r\n\u00e2\u0080\u00a2       Used Oozie to orchestrate the workflow.\r\n\u00e2\u0080\u00a2       Implemented Fair schedulers on the Job tracker to share the resources of the Cluster for the Map Reduce\r\njobs given by the users.\r\n\u00e2\u0080\u00a2       Exported the analyzed data to the relational databases using HIVE for visualization and to generate reports for the BI team.\r\n\r\nEnvironment: Hadoop, Hive, Linux, MapReduce, HDFS, Hive, Python, Pig, Sqoop, Cloudera, Shell Scripting,\r\nJava (JDK 1.6), Java 6, Oracle 10g, PL/SQL, SQL*PLUS",
    "summary": "Education Details Hadoop Developer",
    "category": "Hadoop"
}